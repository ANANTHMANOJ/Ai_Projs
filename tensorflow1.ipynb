{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"amn_2:0\", shape=(3, 6), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "h=tf.constant('AMN',name='amn',shape=[3,6])\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"amn_3:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "h2=tf.constant('AMN',name='amn')\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([[2.1]])\n",
    "b=tf.constant([[3.3]])\n",
    "c=tf.add(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3999996]]\n",
      "[[b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']] b'AMN'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as ss:\n",
    "    print(ss.run(c))\n",
    "    print(ss.run(h),ss.run(h2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3999996]]\n",
      "[[b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']]\n",
      "[[b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']\n",
      " [b'AMN' b'AMN' b'AMN' b'AMN' b'AMN' b'AMN']]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as ss:\n",
    "    print(ss.run(c))\n",
    "    print(ss.run(h))\n",
    "    print(ss.run(h))\n",
    "    ss.close()\n",
    "    #print(ss.run(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "(features,a_pricing),_=tf.keras.datasets.boston_housing.load_data(test_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f9e33121c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f9e33121c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f9e33121c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f9e33121c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(shape=[None,13],name='x',dtype=tf.float32)\n",
    "x_n=tf.layers.batch_normalization(x,training=True)\n",
    "y_=tf.placeholder(shape=[None],name='y',dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.zeros(shape=[13,1]),name='weights')\n",
    "b=tf.Variable(tf.zeros(shape=[1]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.add(tf.matmul(x_n,w),b,name='mat_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.square(y-y_),name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op=tf.train.GradientDescentOptimizer(0.03).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0  loss 592.14703\n",
      "ep 10  loss 231.71227\n",
      "ep 20  loss 121.856804\n",
      "ep 30  loss 84.96823\n",
      "ep 40  loss 84.55544\n",
      "ep 50  loss 84.49244\n",
      "ep 60  loss 84.4652\n",
      "ep 70  loss 84.44976\n",
      "ep 80  loss 84.44013\n",
      "ep 90  loss 84.433846\n",
      "ep 100  loss 84.4296\n",
      "ep 110  loss 84.42671\n",
      "ep 120  loss 84.424706\n",
      "ep 130  loss 84.423294\n",
      "ep 140  loss 84.42232\n",
      "ep 150  loss 84.4216\n",
      "ep 160  loss 84.42108\n",
      "ep 170  loss 84.420715\n",
      "ep 180  loss 84.42045\n",
      "ep 190  loss 84.42027\n",
      "ep 200  loss 84.42013\n",
      "ep 210  loss 84.420006\n",
      "ep 220  loss 84.41992\n",
      "ep 230  loss 84.41987\n",
      "ep 240  loss 84.419815\n",
      "ep 250  loss 84.41978\n",
      "ep 260  loss 84.41975\n",
      "ep 270  loss 84.41973\n",
      "ep 280  loss 84.41971\n",
      "ep 290  loss 84.4197\n",
      "ep 300  loss 84.41968\n",
      "ep 310  loss 84.41968\n",
      "ep 320  loss 84.41967\n",
      "ep 330  loss 84.41965\n",
      "ep 340  loss 84.41964\n",
      "ep 350  loss 84.41963\n",
      "ep 360  loss 84.41963\n",
      "ep 370  loss 84.419624\n",
      "ep 380  loss 84.41961\n",
      "ep 390  loss 84.41961\n",
      "ep 400  loss 84.41959\n",
      "ep 410  loss 84.41959\n",
      "ep 420  loss 84.4196\n",
      "ep 430  loss 84.41958\n",
      "ep 440  loss 84.41958\n",
      "ep 450  loss 84.41958\n",
      "ep 460  loss 84.41958\n",
      "ep 470  loss 84.41958\n",
      "ep 480  loss 84.41958\n",
      "ep 490  loss 84.41956\n",
      "ep 500  loss 84.41957\n",
      "ep 510  loss 84.41957\n",
      "ep 520  loss 84.41956\n",
      "ep 530  loss 84.41956\n",
      "ep 540  loss 84.41956\n",
      "ep 550  loss 84.41956\n",
      "ep 560  loss 84.419556\n",
      "ep 570  loss 84.41956\n",
      "ep 580  loss 84.41956\n",
      "ep 590  loss 84.419556\n",
      "ep 600  loss 84.41955\n",
      "ep 610  loss 84.41955\n",
      "ep 620  loss 84.41955\n",
      "ep 630  loss 84.41955\n",
      "ep 640  loss 84.41955\n",
      "ep 650  loss 84.41955\n",
      "ep 660  loss 84.41955\n",
      "ep 670  loss 84.41955\n",
      "ep 680  loss 84.41955\n",
      "ep 690  loss 84.41955\n",
      "ep 700  loss 84.41955\n",
      "ep 710  loss 84.41954\n",
      "ep 720  loss 84.41955\n",
      "ep 730  loss 84.41955\n",
      "ep 740  loss 84.41955\n",
      "ep 750  loss 84.41955\n",
      "ep 760  loss 84.41955\n",
      "ep 770  loss 84.41953\n",
      "ep 780  loss 84.41953\n",
      "ep 790  loss 84.41953\n",
      "ep 800  loss 84.41953\n",
      "ep 810  loss 84.41953\n",
      "ep 820  loss 84.41953\n",
      "ep 830  loss 84.419525\n",
      "ep 840  loss 84.41953\n",
      "ep 850  loss 84.41954\n",
      "ep 860  loss 84.41953\n",
      "ep 870  loss 84.41953\n",
      "ep 880  loss 84.41953\n",
      "ep 890  loss 84.41953\n",
      "ep 900  loss 84.41953\n",
      "ep 910  loss 84.41953\n",
      "ep 920  loss 84.419525\n",
      "ep 930  loss 84.41953\n",
      "ep 940  loss 84.419525\n",
      "ep 950  loss 84.419525\n",
      "ep 960  loss 84.419525\n",
      "ep 970  loss 84.41952\n",
      "ep 980  loss 84.419525\n",
      "ep 990  loss 84.41952\n"
     ]
    }
   ],
   "source": [
    "ep=1000\n",
    "for e in range(ep):\n",
    "    _,train_l=ss.run([train_op,loss],feed_dict={x:features,y_:a_pricing})\n",
    "    if e%10==0:\n",
    "        print('ep',e,' loss',train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.BatchNormalization(input_shape=(13,)))\n",
    "model.add(keras.layers.Dense(1))# 1=number of output\n",
    "model.compile(optimizer='sgd',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "506/506 [==============================] - 0s 559us/sample - loss: 194.9395\n",
      "Epoch 2/100\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 51.9537\n",
      "Epoch 3/100\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 38.1916\n",
      "Epoch 4/100\n",
      "506/506 [==============================] - 0s 66us/sample - loss: 34.6261\n",
      "Epoch 5/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 35.4483\n",
      "Epoch 6/100\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 32.7088\n",
      "Epoch 7/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 33.9047\n",
      "Epoch 8/100\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 29.6537\n",
      "Epoch 9/100\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 29.4620\n",
      "Epoch 10/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 28.3213\n",
      "Epoch 11/100\n",
      "506/506 [==============================] - 0s 62us/sample - loss: 27.3437\n",
      "Epoch 12/100\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 30.5377\n",
      "Epoch 13/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 25.9897\n",
      "Epoch 14/100\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 24.9737\n",
      "Epoch 15/100\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 30.0140\n",
      "Epoch 16/100\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 29.4927\n",
      "Epoch 17/100\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 33.4636\n",
      "Epoch 18/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 27.2728\n",
      "Epoch 19/100\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 27.9695\n",
      "Epoch 20/100\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 28.3202\n",
      "Epoch 21/100\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 27.1072\n",
      "Epoch 22/100\n",
      "506/506 [==============================] - 0s 66us/sample - loss: 28.3993\n",
      "Epoch 23/100\n",
      "506/506 [==============================] - 0s 82us/sample - loss: 27.7876\n",
      "Epoch 24/100\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 28.3409\n",
      "Epoch 25/100\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 26.8361\n",
      "Epoch 26/100\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 26.9031\n",
      "Epoch 27/100\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 26.8380\n",
      "Epoch 28/100\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 26.9285\n",
      "Epoch 29/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 26.0820\n",
      "Epoch 30/100\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 30.9472\n",
      "Epoch 31/100\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 27.3181\n",
      "Epoch 32/100\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 32.3884\n",
      "Epoch 33/100\n",
      "506/506 [==============================] - 0s 74us/sample - loss: 28.6179\n",
      "Epoch 34/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 29.4081\n",
      "Epoch 35/100\n",
      "506/506 [==============================] - 0s 76us/sample - loss: 28.5039\n",
      "Epoch 36/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 29.0651\n",
      "Epoch 37/100\n",
      "506/506 [==============================] - 0s 75us/sample - loss: 28.5648\n",
      "Epoch 38/100\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 29.6659\n",
      "Epoch 39/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 24.7157\n",
      "Epoch 40/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 29.6022\n",
      "Epoch 41/100\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 26.1491\n",
      "Epoch 42/100\n",
      "506/506 [==============================] - 0s 85us/sample - loss: 30.4715\n",
      "Epoch 43/100\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 23.6830\n",
      "Epoch 44/100\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 25.1235\n",
      "Epoch 45/100\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 28.0651\n",
      "Epoch 46/100\n",
      "506/506 [==============================] - 0s 78us/sample - loss: 27.3236\n",
      "Epoch 47/100\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 29.9258\n",
      "Epoch 48/100\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 28.7975\n",
      "Epoch 49/100\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 31.9212\n",
      "Epoch 50/100\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 26.6371\n",
      "Epoch 51/100\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 29.4906\n",
      "Epoch 52/100\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 26.9012\n",
      "Epoch 53/100\n",
      "506/506 [==============================] - 0s 106us/sample - loss: 32.2253\n",
      "Epoch 54/100\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 29.0859\n",
      "Epoch 55/100\n",
      "506/506 [==============================] - 0s 92us/sample - loss: 27.2414\n",
      "Epoch 56/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 29.1328\n",
      "Epoch 57/100\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 29.7442\n",
      "Epoch 58/100\n",
      "506/506 [==============================] - 0s 70us/sample - loss: 25.3578\n",
      "Epoch 59/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 28.7293\n",
      "Epoch 60/100\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 27.4059\n",
      "Epoch 61/100\n",
      "506/506 [==============================] - 0s 67us/sample - loss: 27.3753\n",
      "Epoch 62/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 27.9565\n",
      "Epoch 63/100\n",
      "506/506 [==============================] - 0s 58us/sample - loss: 29.6732\n",
      "Epoch 64/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 26.1049\n",
      "Epoch 65/100\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 30.1082\n",
      "Epoch 66/100\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 26.4941\n",
      "Epoch 67/100\n",
      "506/506 [==============================] - 0s 64us/sample - loss: 30.0713\n",
      "Epoch 68/100\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 29.4602\n",
      "Epoch 69/100\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 24.3431\n",
      "Epoch 70/100\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 25.6954\n",
      "Epoch 71/100\n",
      "506/506 [==============================] - 0s 61us/sample - loss: 32.8005\n",
      "Epoch 72/100\n",
      "506/506 [==============================] - 0s 77us/sample - loss: 29.4427\n",
      "Epoch 73/100\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 30.9586\n",
      "Epoch 74/100\n",
      "506/506 [==============================] - 0s 65us/sample - loss: 29.4332\n",
      "Epoch 75/100\n",
      "506/506 [==============================] - 0s 66us/sample - loss: 25.2657\n",
      "Epoch 76/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 28.2905\n",
      "Epoch 77/100\n",
      "506/506 [==============================] - 0s 89us/sample - loss: 26.1041\n",
      "Epoch 78/100\n",
      "506/506 [==============================] - 0s 69us/sample - loss: 25.5012\n",
      "Epoch 79/100\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 32.9350\n",
      "Epoch 80/100\n",
      "506/506 [==============================] - 0s 103us/sample - loss: 31.9525\n",
      "Epoch 81/100\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 29.0077\n",
      "Epoch 82/100\n",
      "506/506 [==============================] - 0s 79us/sample - loss: 28.2733\n",
      "Epoch 83/100\n",
      "506/506 [==============================] - 0s 88us/sample - loss: 27.9684\n",
      "Epoch 84/100\n",
      "506/506 [==============================] - 0s 73us/sample - loss: 26.4372\n",
      "Epoch 85/100\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 28.5513\n",
      "Epoch 86/100\n",
      "506/506 [==============================] - 0s 90us/sample - loss: 25.6047\n",
      "Epoch 87/100\n",
      "506/506 [==============================] - 0s 81us/sample - loss: 25.9202\n",
      "Epoch 88/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 28.1719\n",
      "Epoch 89/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 28.9239\n",
      "Epoch 90/100\n",
      "506/506 [==============================] - 0s 95us/sample - loss: 30.5399\n",
      "Epoch 91/100\n",
      "506/506 [==============================] - 0s 71us/sample - loss: 29.3407\n",
      "Epoch 92/100\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 25.9056\n",
      "Epoch 93/100\n",
      "506/506 [==============================] - 0s 63us/sample - loss: 29.3427\n",
      "Epoch 94/100\n",
      "506/506 [==============================] - 0s 68us/sample - loss: 23.9336\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 74us/sample - loss: 28.7281\n",
      "Epoch 96/100\n",
      "506/506 [==============================] - 0s 93us/sample - loss: 28.0898\n",
      "Epoch 97/100\n",
      "506/506 [==============================] - 0s 83us/sample - loss: 27.8306\n",
      "Epoch 98/100\n",
      "506/506 [==============================] - 0s 91us/sample - loss: 29.0100\n",
      "Epoch 99/100\n",
      "506/506 [==============================] - 0s 72us/sample - loss: 29.9082\n",
      "Epoch 100/100\n",
      "506/506 [==============================] - 0s 80us/sample - loss: 25.2258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e331db898>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features,a_pricing,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
